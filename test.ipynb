{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.abspath('/home/gzou/brxngenerator-master/rxnft_vae'))\n",
    "\n",
    "import rdkit\n",
    "import rdkit.Chem as Chem\n",
    "from rdkit.Chem import QED, Descriptors, rdmolops\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math, random, sys\n",
    "from optparse import OptionParser\n",
    "from collections import deque\n",
    "import pickle as pickle\n",
    "import yaml\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "from rxnft_vae.reaction_utils import get_mol_from_smiles, get_smiles_from_mol,read_multistep_rxns, get_template_order, get_qed_score,get_clogp_score\n",
    "from rxnft_vae.reaction import ReactionTree, extract_starting_reactants, StartingReactants, Templates, extract_templates,stats\n",
    "from rxnft_vae.fragment import FragmentVocab, FragmentTree, FragmentNode, can_be_decomposed\n",
    "from rxnft_vae.vae import FTRXNVAE, set_batch_nodeID, bFTRXNVAE\n",
    "from rxnft_vae.mpn import MPN,PP,Discriminator\n",
    "import rxnft_vae.sascorer as sascorer\n",
    "import random\n",
    "\n",
    "from rdkit import Chem\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from amplify import BinaryMatrix, BinaryPoly, gen_symbols, sum_poly\n",
    "from amplify import decode_solution, Solver\n",
    "from amplify.client import FixstarsClient\n",
    "from amplify.client.ocean import DWaveSamplerClient\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "UPDATE_ITER = 1\n",
    "\n",
    "class TorchFM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n=None, k=None):\n",
    "        # n: size of binary features\n",
    "        # k: size of latent features\n",
    "        super().__init__()\n",
    "        self.V = nn.Parameter(torch.randn(n, k), requires_grad=True)\n",
    "        self.lin = nn.Linear(n, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_1 = torch.matmul(x, self.V).pow(2).sum(1, keepdim=True) #S_1^2\n",
    "        out_2 = torch.matmul(x.pow(2), self.V.pow(2)).sum(1, keepdim=True) # S_2\n",
    "        \n",
    "        out_inter = 0.5*(out_1 - out_2)\n",
    "        out_lin = self.lin(x)\n",
    "        out = out_inter + out_lin\n",
    "        out = out.squeeze(dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MolData(Dataset):\n",
    "    \n",
    "    def __init__(self, binary, targets):\n",
    "        self.binary = binary\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.binary)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.binary[index], self.targets[index]\n",
    "\n",
    "\n",
    "class RandomBinaryData(Dataset):\n",
    "\n",
    "    def __init__(self, binary):\n",
    "        self.binary = binary\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.binary)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.binary[index]\n",
    "\n",
    "\n",
    "class bVAE_IM(object):\n",
    "    def __init__(self, bvae_model=None, smiles=None, targets=None, seed=0, n_sample=1):\n",
    "\n",
    "        self.bvae_model = bvae_model\n",
    "        self.train_smiles = smiles\n",
    "        self.train_targets = targets\n",
    "\n",
    "        self.random_seed = seed\n",
    "        if self.random_seed is not None:\n",
    "            seed_all(self.random_seed)\n",
    "        \n",
    "        self.n_sample = n_sample # configs['opt']['n_sample']\n",
    "        # self._initialize()\n",
    "        self.sleep_count = 0\n",
    "        \n",
    "    def decode_many_times(self, latent):\n",
    "        binary = F.one_hot(latent.long(),num_classes=2).float()\n",
    "        binary = binary.view(1, -1)\n",
    "        prob_decode = True\n",
    "        binary_size = self.bvae_model.binary_size\n",
    "        # ft_mean = latent[:, :latent_size]\n",
    "        # rxn_mean = latent[:, latent_size:]\n",
    "        ft_mean = binary[:, :binary_size*2]\n",
    "        rxn_mean = binary[:, binary_size*2:]\n",
    "        product_list=[]\n",
    "        for i in range(50):\n",
    "            generated_tree = self.bvae_model.fragment_decoder.decode(ft_mean, prob_decode)\n",
    "            g_encoder_output, g_root_vec = self.bvae_model.fragment_encoder([generated_tree])\n",
    "            product, reactions = self.bvae_model.rxn_decoder.decode(rxn_mean, g_encoder_output, prob_decode)\n",
    "            if product != None:\n",
    "                product_list.append([product, reactions])\n",
    "            # break\n",
    "        if len(product_list) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return product_list\n",
    "\n",
    "\n",
    "    def optimize(self, X_train, y_train, X_test, y_test, configs):\n",
    "        \n",
    "        n_opt = 100 # configs['opt']['num_end']\n",
    "        self.train_binary = torch.vstack((X_train, X_test))\n",
    "        self.n_binary = self.train_binary.shape[1]\n",
    "        \n",
    "        self.valid_smiles =[]\n",
    "        self.new_features =[]\n",
    "        self.full_rxn_strs=[]\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "        self.end_cond = configs['opt']['end_cond']\n",
    "        if self.end_cond not in [0, 1, 2]:\n",
    "            raise ValueError(\"end_cond should be 0, 1 or 2.\")\n",
    "        if self.end_cond == 2:\n",
    "            n_opt = 100 # n_opt is patience in this condition. When patience exceeds 100, exhaustion searching ends.\n",
    "\n",
    "        self.results_smiles = []\n",
    "        self.results_binary = []\n",
    "        self.results_scores = []\n",
    "\n",
    "        # # config Ising machine\n",
    "\n",
    "        # client = configs['opt']['client']\n",
    "        client = FixstarsClient()\n",
    "        client.token = configs['opt']['client_token']\n",
    "        client.parameters.timeout = 1000\n",
    "        # elif client == \"dwave\":\n",
    "        #     client = DWaveSamplerClient()\n",
    "        #     client.token = configs['opt']['client_token']\n",
    "        #     client.solver = configs['opt']['dwave_sys']\n",
    "        #     client.parameters.num_reads = 1000\n",
    "        #     client.parameters.max_answers = 1\n",
    "        # else:\n",
    "        #     raise ValueError(\"Wrong client!\")\n",
    "\n",
    "        solver = Solver(client)\n",
    "\n",
    "        self.iteration = 0\n",
    "\n",
    "        while self.iteration < n_opt:\n",
    "\n",
    "            # train factorization machine\n",
    "            qubo = self._build_qubo(X_train, X_test, y_train, y_test, configs)\n",
    "\n",
    "            solution, energy = self._solve_qubo(qubo = qubo,\n",
    "                                    qubo_solver = solver)\n",
    "\n",
    "            # merge new data into dataset\n",
    "            self._update(solution=solution,\n",
    "                        energy=energy)\n",
    "\n",
    "        result_save_dir = configs['opt']['output']\n",
    "        if not os.path.exists(result_save_dir):\n",
    "            os.mkdir(result_save_dir)\n",
    "        \n",
    "        with open((os.path.join(result_save_dir, \"%s_smiles.pkl\" % configs['opt']['prop'])), \"wb\") as f:\n",
    "            pickle.dump(self.results_smiles, f)\n",
    "        with open((os.path.join(result_save_dir, \"%s_scores.pkl\" % configs['opt']['prop'])), \"wb\") as f:\n",
    "            pickle.dump(self.results_scores, f)\n",
    "        \n",
    "        logging.info(\"Sleeped for %d minutes...\" % self.sleep_count)\n",
    "        \n",
    "\n",
    "    # def _initialize(self):\n",
    "    #     self.train_smiles = self.train_smiles.tolist()\n",
    "    #     self.train_targets = self.train_targets.astype('float')\n",
    "    #     self.train_mols = [Chem.MolFromSmiles(s) for s in self.train_smiles]\n",
    "\n",
    "    #     self.train_binary = self._encode_to_binary(self.train_smiles)\n",
    "\n",
    "    #     if self.opt_target == 'max':\n",
    "    #         self.train_targets = [-self.get_score(m) for m in self.train_mols]\n",
    "    #     elif self.opt_target == 'min':\n",
    "    #         self.train_targets = [self.get_score(m) for m in self.train_mols]\n",
    "    #     self.train_targets = np.repeat(self.train_targets, self.n_sample).tolist()\n",
    "    #     # plus --> minimization; minus --> maximization\n",
    "\n",
    "    # def _encode_to_binary(self, smiles, batch_size = 64):\n",
    "    #     encoded = []\n",
    "    #     print(\"encoding molecules to binary sequences...\")\n",
    "    #     for i in tqdm(range(int(np.ceil(len(smiles) / batch_size)))):\n",
    "    #         smiles_batch = smiles[i*batch_size: (i+1)*batch_size]\n",
    "    #         if self.n_sample == 1:\n",
    "    #             encoded_batch = self.bvae_model.encode_from_smiles(smiles_batch)\n",
    "    #         else:\n",
    "    #             encoded_batch = self.bvae_model.encode_from_smiles(smiles_batch, self.n_sample)\n",
    "    #         encoded.append(encoded_batch)\n",
    "    #     train_binary = torch.vstack(encoded)\n",
    "    #     train_binary = train_binary.to('cpu').numpy()\n",
    "    #     return train_binary\n",
    "\n",
    "    def _build_qubo(self, X_train, X_valid, y_train, y_valid, configs):\n",
    "        \n",
    "        model = TorchFM(self.n_binary, configs['opt']['factor_num'])# .to(self.device)\n",
    "        for param in model.parameters():\n",
    "            if param.dim() == 1:\n",
    "                nn.init.constant_(param, 0)     # bias\n",
    "            else:\n",
    "                nn.init.uniform_(param, -configs['opt']['param_init'], configs['opt']['param_init'])   # weights\n",
    "\n",
    "        # X_train, X_valid, y_train, y_valid = train_test_split(self.train_binary,\n",
    "        #                                                 self.train_targets,\n",
    "        #                                                 test_size=0.1,\n",
    "        #                                                 random_state=self.iteration)\n",
    "\n",
    "        # X_train = torch.from_numpy(X_train).to(torch.float).to(self.device)\n",
    "        # X_valid = torch.from_numpy(X_valid).to(torch.float).to(self.device)\n",
    "        # y_train = torch.tensor(y_train).to(torch.float).to(self.device)\n",
    "        # y_valid = torch.tensor(y_valid).to(torch.float).to(self.device)\n",
    "\n",
    "        print('========shape: ', X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "        dataset_train = MolData(X_train, y_train)\n",
    "        dataloader_train = DataLoader(dataset=dataset_train,\n",
    "                                    batch_size=configs['opt']['batch_size'],\n",
    "                                    shuffle=True)\n",
    "        dataset_valid = MolData(X_valid, y_valid)\n",
    "        dataloader_valid = DataLoader(dataset=dataset_valid,\n",
    "                                    batch_size=configs['opt']['batch_size'],\n",
    "                                    shuffle=False)\n",
    "\n",
    "        print('lr: ', configs['opt']['lr'])\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                        lr=configs['opt']['lr'],\n",
    "                                        weight_decay=configs['opt']['decay_weight'])\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        lowest_error = float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(configs['opt']['maxepoch']):\n",
    "            model.train()\n",
    "            for batch_x, batch_y in dataloader_train:\n",
    "                optimizer.zero_grad()\n",
    "                out = model(batch_x)\n",
    "                loss = criterion(out, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_hat_valid = []\n",
    "                for batch_x, _ in dataloader_valid:\n",
    "                    valid = model(batch_x)\n",
    "                    y_hat_valid.append(valid)\n",
    "                y_hat_valid = torch.cat(y_hat_valid)\n",
    "\n",
    "                epoch_error = criterion(y_valid, y_hat_valid)\n",
    "                epoch_error = epoch_error.detach().cpu().numpy()\n",
    "                if epoch % 100 == 0:\n",
    "                    print(\"Model -- Epoch %d error on validation set: %.4f\" % (epoch, epoch_error))\n",
    "                \n",
    "                if epoch_error < lowest_error:\n",
    "                    torch.save(model.state_dict(),\n",
    "                                os.path.join(configs['opt']['cache'],\n",
    "                                \"fm_model-%s-%s-dim%d-seed%d-end%d\" % (\n",
    "                                    configs['opt']['prop'],\n",
    "                                    configs['opt']['client'],\n",
    "                                    self.n_binary,\n",
    "                                    self.random_seed,\n",
    "                                    self.end_cond)))\n",
    "                    lowest_error = epoch_error\n",
    "                    best_epoch = epoch\n",
    "\n",
    "                if epoch > best_epoch+configs['opt']['patience']:\n",
    "                    print(\"Model -- Epoch %d has lowest error!\" % (best_epoch))\n",
    "                    break\n",
    "        \n",
    "        y_hat_valid = y_hat_valid.unsqueeze(1).detach().cpu().numpy()\n",
    "        y_valid = y_valid.detach().cpu().numpy()\n",
    "        print(y_hat_valid.shape, y_valid.shape)\n",
    "        # print(np.corrcoef(y_hat_valid, y_valid))\n",
    "        # reload best epoch\n",
    "        model.load_state_dict(torch.load(\n",
    "            os.path.join(configs['opt']['cache'],\n",
    "            \"fm_model-%s-%s-dim%d-seed%d-end%d\" % (\n",
    "                configs['opt']['prop'],\n",
    "                configs['opt']['client'],\n",
    "                self.n_binary,\n",
    "                self.random_seed,\n",
    "                self.end_cond)))\n",
    "            )\n",
    "\n",
    "        for p in model.parameters():\n",
    "            if tuple(p.shape) == (self.n_binary, configs['opt']['factor_num']):\n",
    "                Vi_f= p.to(\"cpu\").detach().numpy()\n",
    "            elif tuple(p.shape) == (1, self.n_binary):\n",
    "                Wi = p.to(\"cpu\").detach().numpy()\n",
    "            elif tuple(p.shape) == (1, ):\n",
    "                W0 = p.to(\"cpu\").detach().numpy()\n",
    "\n",
    "        # build the QUBO graph\n",
    "        q = gen_symbols(BinaryPoly, self.n_binary)\n",
    "        f_E = sum_poly(configs['opt']['factor_num'], lambda f: ((sum_poly(self.n_binary, lambda i: Vi_f[i][f] * q[i]))**2 - sum_poly(self.n_binary, lambda i: Vi_f[i][f]**2 * q[i]**2)))/2 \\\n",
    "            + sum_poly(self.n_binary, lambda i: Wi[0][i]*q[i]) \\\n",
    "            + W0[0]\n",
    "        qubo = (q, f_E)\n",
    "\n",
    "        return qubo\n",
    "\n",
    "\n",
    "    def _solve_qubo(self,\n",
    "        qubo,\n",
    "        qubo_solver):\n",
    "\n",
    "        if isinstance(qubo, tuple):\n",
    "            q, qubo = qubo\n",
    "\n",
    "        solved = False\n",
    "        while not solved:\n",
    "            try:\n",
    "                result = qubo_solver.solve(qubo)\n",
    "                solved = True\n",
    "            except RuntimeError as e: # retry after 60s if connection to the solver fails..\n",
    "                time.sleep(60)\n",
    "                self.sleep_count += 1\n",
    "\n",
    "        sols = []\n",
    "        sol_E = []\n",
    "        for sol in result:  # Iterate over multiple solutions\n",
    "            # solution = [sol.values[i] for i in range(self.n_binary)]\n",
    "            if isinstance(qubo, BinaryMatrix):\n",
    "                solution = [sol.values[i] for i in range(self.n_binary)]\n",
    "            elif isinstance(qubo, BinaryPoly):\n",
    "                solution = decode_solution(q, sol.values)\n",
    "            else:\n",
    "                raise ValueError(\"qubo type unknown!\")\n",
    "            sols.append(solution)\n",
    "            sol_E.append(sol.energy)\n",
    "        return np.array(sols), np.array(sol_E).astype(np.float32)\n",
    "\n",
    "    def _update(self,\n",
    "        solution,\n",
    "        energy):\n",
    "\n",
    "        # 0 --> certain number of iterations;\n",
    "        # 1 --> certain number of new molecule;\n",
    "        # 2 --> exhaustion\n",
    "        if self.end_cond == 0:\n",
    "            self.iteration += 1\n",
    "\n",
    "        binary_new = torch.from_numpy(solution).to(torch.float)# .to(self.device)\n",
    "        # smiles_new = self.bvae_model.decode_from_binary(binary_new)   # 1 x 1\n",
    "        # mol_new = Chem.MolFromSmiles(smiles_new)\n",
    "        print('========binary_new shape is', binary_new.shape)\n",
    "        res= self.decode_many_times(binary_new)\n",
    "        if res is not None:\n",
    "            smiles_list = [re[0] for re in res]\n",
    "            n_reactions = [len(re[1].split(\" \")) for re in res]\n",
    "            #print(n_reactions)\n",
    "            for re in res:\n",
    "                smiles = re[0]\n",
    "                if len(re[1].split(\" \")) > 0 and smiles not in self.valid_smiles and smiles is not None:\n",
    "                    #print(smiles, re[1].split(\" \"))\n",
    "                    self.valid_smiles.append(smiles)\n",
    "                    self.new_features.append(latent)\n",
    "                    self.full_rxn_strs.append(re[1])\n",
    "        \n",
    "        #new_features = np.vstack(new_features)\n",
    "        print('========cal new score')\n",
    "        scores =[]\n",
    "        b_valid_smiles=[]\n",
    "        b_full_rxn_strs=[]\n",
    "        b_scores=[]\n",
    "        # b_new_features=[]\n",
    "        for i in range(len(self.valid_smiles)):\n",
    "            if metric ==\"logp\":\n",
    "                mol = rdkit.Chem.MolFromSmiles(self.valid_smiles[i])\n",
    "                if mol is None:\n",
    "                    continue\n",
    "                current_log_P_value = Descriptors.MolLogP(mol)\n",
    "                current_SA_score = -sascorer.calculateScore(mol)\n",
    "                cycle_list = nx.cycle_basis(nx.Graph(rdmolops.GetAdjacencyMatrix(mol)))\n",
    "                if len(cycle_list) == 0:\n",
    "                    cycle_length = 0\n",
    "                else:\n",
    "                    cycle_length = max([ len(j) for j in cycle_list ])\n",
    "                if cycle_length <= 6:\n",
    "                    cycle_length = 0\n",
    "                else:\n",
    "                    cycle_length = cycle_length - 6\n",
    "                current_cycle_score = -cycle_length\n",
    "                current_SA_score_normalized = (current_SA_score - sascore_m) / sascore_s\n",
    "                current_log_P_value_normalized = (current_log_P_value - logp_m) / logp_s\n",
    "                current_cycle_score_normalized = (current_cycle_score - cycle_m) / cycle_s\n",
    "                score = current_SA_score_normalized + current_log_P_value_normalized + current_cycle_score_normalized\n",
    "                scores.append(-score)\n",
    "                b_valid_smiles.append(self.valid_smiles[i])\n",
    "                b_full_rxn_strs.append(self.full_rxn_strs[i])\n",
    "                # b_new_features.append(new_features[i])\n",
    "            if metric==\"qed\":\n",
    "                mol = rdkit.Chem.MolFromSmiles(self.valid_smiles[i])\n",
    "                if mol!=None:\n",
    "                    score = QED.qed(mol)\n",
    "                    scores.append(-score)\n",
    "                    b_valid_smiles.append(self.valid_smiles[i])\n",
    "                    b_full_rxn_strs.append(self.full_rxn_strs[i])\n",
    "                    # b_new_features.append(new_features[i])\n",
    "        print('========scores shape is', len(scores))\n",
    "        # new_features = np.vstack(b_new_features)\n",
    "        if len(scores) > 0:\n",
    "            self.X_train = np.concatenate([ self.X_train, binary_new ], 0)\n",
    "            self.y_train = np.concatenate([ self.y_train, np.array(scores)[ :, None ] ], 0)\n",
    "            print(f\"X_train shape: {self.X_train.shape}, y_train shape: {self.y_train.shape}\")\n",
    "\n",
    "        # for i in range(len(b_valid_smiles)):\n",
    "        #     line = \" \".join([b_valid_smiles[i], b_full_rxn_strs[i], str(scores[i])])\n",
    "        #     writer.write(line + \"\\n\")\n",
    "\n",
    "        # # skip invalid smiles\n",
    "        # if mol_new is None:\n",
    "        #     return\n",
    "\n",
    "        # if smiles_new in self.train_smiles:\n",
    "        #     if self.end_cond == 2:\n",
    "        #         self.iteration += 1\n",
    "        #     return\n",
    "\n",
    "        # # fm_pred = fm_model(binary_new)\n",
    "        # # fm_pred = fm_pred.detach().cpu().numpy()\n",
    "\n",
    "        # # assert np.round(fm_pred, 3) == np.round(energy[0], 3)    # ensure correctness of qubo\n",
    "        # if self.opt_target == 'max':\n",
    "        #     target_new = -self.get_score(mol_new)\n",
    "        # else:\n",
    "        #     target_new = self.get_score(mol_new)\n",
    "        # print(\"energy: %.3f; target: %.3f\" % (energy[0], target_new))\n",
    "        # self.train_smiles.append(smiles_new)\n",
    "        # # self.train_binary = torch.vstack((self.train_binary, binary_new))\n",
    "        # binary_new = binary_new.to('cpu').numpy()\n",
    "        # self.train_binary = np.vstack((self.train_binary, binary_new))\n",
    "        # print(self.train_binary.shape)\n",
    "        # self.train_targets.append(target_new)\n",
    "\n",
    "\n",
    "        # # if new molecule is generated:\n",
    "        # if self.end_cond == 1:\n",
    "        #     self.iteration += 1\n",
    "        # if self.end_cond == 2:\n",
    "        #     self.iteration = 0 # if new molecule is generated, reset to 0\n",
    "\n",
    "        # self.results_smiles.append(smiles_new)\n",
    "        # self.results_binary.extend(solution)\n",
    "        # self.results_scores.append(-target_new)\n",
    "\n",
    "        # print(self.X_train.shape, self.y_train.shape, np.array(scores)[ :, None ].shape)\n",
    "        # logging.info(\"Iteration %d: QUBO energy -- %.4f, actual energy -- %.4f, smiles -- %s\" % (self.iteration, energy[0]))\n",
    "        \n",
    "        assert self.X_train.shape[0] == self.y_train.shape[0]\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "def main(X_train, y_train, X_test, y_test, smiles, targets, model, parameters, configs, metric, seed):\n",
    "    X_train = torch.Tensor(X_train)\n",
    "    y_train = torch.Tensor(y_train)\n",
    "    X_test = torch.Tensor(X_test)\n",
    "    y_test = torch.Tensor(y_test)\n",
    "\n",
    "    optimizer = bVAE_IM(smiles=smiles, targets=targets, bvae_model=model, seed=seed)\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    optimizer.optimize(X_train, y_train, X_test, y_test, configs)\n",
    "\n",
    "    logging.info(\"Running Time: %f\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "def seed_all(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get parameters\n",
    "hidden_size = int(200)\n",
    "latent_size = int(50)\n",
    "depth = int(2)\n",
    "seed = int(100)\n",
    "vocab_path = 'weights/data.txt_fragmentvocab.txt'\n",
    "data_filename = 'data/data.txt'\n",
    "w_save_path = 'data/data.txt_vae_iter-100.npy'\n",
    "metric = 'qed'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-PCIE-40GB\n",
      "hidden size: 200 latent_size: 50 depth: 2\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "if torch.cuda.is_available():\n",
    "#device = torch.device(\"cuda:1\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.set_device(0)\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "print(\"hidden size:\", hidden_size, \"latent_size:\", latent_size, \"depth:\", depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data.....\n",
      "size of reactant dic: 9766\n",
      "size of template dic: 5567\n"
     ]
    }
   ],
   "source": [
    "print(\"loading data.....\")\n",
    "routes, scores = read_multistep_rxns(data_filename)\n",
    "# routes = routes[:10]\n",
    "# scores = scores[:10]\n",
    "rxn_trees = [ReactionTree(route) for route in routes]\n",
    "molecules = [rxn_tree.molecule_nodes[0].smiles for rxn_tree in rxn_trees]\n",
    "reactants = extract_starting_reactants(rxn_trees)\n",
    "templates, n_reacts = extract_templates(rxn_trees)\n",
    "reactantDic = StartingReactants(reactants)\n",
    "templateDic = Templates(templates, n_reacts)\n",
    "\n",
    "print(\"size of reactant dic:\", reactantDic.size())\n",
    "print(\"size of template dic:\", templateDic.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of fragment dic: 275\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "n_pairs = len(routes)\n",
    "ind_list = [i for i in range(n_pairs)]\n",
    "# fgm_trees = [FragmentTree(rxn_trees[i].molecule_nodes[0].smiles) for i in ind_list]\n",
    "fgm_trees = pickle.load(open('fgm_trees.pkl', 'rb'))\n",
    "rxn_trees = [rxn_trees[i] for i in ind_list]\n",
    "data_pairs=[]\n",
    "for fgm_tree, rxn_tree in zip(fgm_trees, rxn_trees):\n",
    "    data_pairs.append((fgm_tree, rxn_tree))\n",
    "cset=set()\n",
    "for fgm_tree in fgm_trees:\n",
    "    for node in fgm_tree.nodes:\n",
    "        cset.add(node.smiles)\n",
    "cset = list(cset)\n",
    "\n",
    "fragmentDic = pickle.load(open('fragmentDic.pkl', 'rb'))\n",
    "\n",
    "# if vocab_path is None:\n",
    "#     fragmentDic = FragmentVocab(cset)\n",
    "# else:\n",
    "#     fragmentDic = FragmentVocab(cset, filename =vocab_path)\n",
    "\n",
    "print(\"size of fragment dic:\", fragmentDic.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzou/micromamba/envs/myconda/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/gzou/micromamba/envs/myconda/lib/python3.8/site-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/tmp/ipykernel_218595/1197526968.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(w_save_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loading model...\n",
      "number of samples: 21218\n",
      "num of samples: 21218\n"
     ]
    }
   ],
   "source": [
    "# loading model\n",
    "\n",
    "mpn = MPN(hidden_size, depth)\n",
    "model = bFTRXNVAE(fragmentDic, reactantDic, templateDic, hidden_size, latent_size, depth, device, fragment_embedding=None, reactant_embedding=None, template_embedding=None)\n",
    "checkpoint = torch.load(w_save_path, map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "print(\"finished loading model...\")\n",
    "\n",
    "print(\"number of samples:\", len(data_pairs))\n",
    "data_pairs = data_pairs\n",
    "latent_list=[]\n",
    "score_list=[]\n",
    "print(\"num of samples:\", len(rxn_trees))\n",
    "latent_list =[]\n",
    "score_list=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples processed: 10\n"
     ]
    }
   ],
   "source": [
    "data_pairs = data_pairs[:10]\n",
    "if metric == \"qed\":\n",
    "    latent = None\n",
    "    # 批量编码所有数据对\n",
    "    prelatent_list = model.encode(data_pairs)\n",
    "    \n",
    "    for i in range(len(prelatent_list)):\n",
    "        latent_list.append(prelatent_list[i])\n",
    "\n",
    "    # 提取所有的 rxn_tree 和 smiles\n",
    "    rxn_trees = [data_pair[1] for data_pair in data_pairs]\n",
    "    smiles_list = [rxn_tree.molecule_nodes[0].smiles for rxn_tree in rxn_trees]\n",
    "\n",
    "    # 批量计算 QED 分数\n",
    "    score_list = [get_qed_score(smiles) for smiles in smiles_list]\n",
    "\n",
    "    print(\"Total samples processed:\", len(score_list))\n",
    "\n",
    "# prelatent_list = pickle.load(open('prelatent_list.pkl', 'rb'))\n",
    "# latent_list = pickle.load(open('latent_list.pkl', 'rb'))\n",
    "# latents = pickle.load(open('latents.pkl', 'rb'))\n",
    "# scores = pickle.load(open('scores.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 11\n",
      "1 12\n",
      "2 13\n",
      "3 14\n",
      "4 15\n",
      "5 16\n",
      "6 17\n",
      "7 18\n",
      "8 19\n",
      "9 20\n"
     ]
    }
   ],
   "source": [
    "if metric ==\"qed\":\n",
    "    for i, data_pair in enumerate(data_pairs):\n",
    "        latent = model.encode([data_pair])\n",
    "        #print(i, latent.size(), latent)\n",
    "        latent_list.append(latent[0])\n",
    "        rxn_tree = data_pair[1]\n",
    "        smiles = rxn_tree.molecule_nodes[0].smiles\n",
    "        score_list.append(get_qed_score(smiles))\n",
    "        print(i, len(score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== 20\n"
     ]
    }
   ],
   "source": [
    "latents = torch.stack(latent_list, dim=0)\n",
    "scores = np.array(score_list)\n",
    "scores = scores.reshape((-1,1))\n",
    "latents = latents.detach().numpy()\n",
    "n = latents.shape[0]\n",
    "print('===================', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# with open('fgm_trees.pkl', 'wb') as f:\n",
    "#     pickle.dump(fgm_trees, f)\n",
    "\n",
    "# # 保存变量\n",
    "# with open('fragmentDic.pkl', 'wb') as f:\n",
    "#     pickle.dump(fragmentDic, f)\n",
    "\n",
    "# with open('latent_list.pkl', 'wb') as f:\n",
    "#     pickle.dump(latent_list, f)\n",
    "\n",
    "# # prelatent_list\n",
    "# with open('prelatent_list.pkl', 'wb') as f:\n",
    "#     pickle.dump(prelatent_list, f)\n",
    "\n",
    "# # latent_list\n",
    "# with open('latents.pkl', 'wb') as f:\n",
    "#     pickle.dump(latents, f)\n",
    "\n",
    "# # scores\n",
    "# with open('scores.pkl', 'wb') as f:\n",
    "#     pickle.dump(scores, f)\n",
    "# # 加载变量\n",
    "# with open('data.pkl', 'rb') as f:\n",
    "#     my_variable = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 50) (2, 50) (18, 1) (2, 1)\n",
      "========shape:  torch.Size([18, 50]) torch.Size([18, 1]) (2, 50) (2, 1)\n",
      "lr:  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzou/micromamba/envs/myconda/lib/python3.8/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/gzou/micromamba/envs/myconda/lib/python3.8/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([2, 1])) that is different to the input size (torch.Size([2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/gzou/micromamba/envs/myconda/lib/python3.8/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model -- Epoch 0 error on validation set: 0.2768\n",
      "Model -- Epoch 100 error on validation set: 0.0097\n",
      "Model -- Epoch 200 error on validation set: 0.0149\n",
      "Model -- Epoch 300 error on validation set: 0.0128\n",
      "Model -- Epoch 80 has lowest error!\n",
      "(2, 1) (2, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_218595/440249758.py:261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========binary_new shape is torch.Size([1, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gzou/micromamba/envs/myconda/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========cal new score\n",
      "========scores shape is 0\n",
      "========shape:  torch.Size([18, 50]) torch.Size([18, 1]) (2, 50) (2, 1)\n",
      "lr:  0.001\n",
      "Model -- Epoch 0 error on validation set: 0.2419\n",
      "Model -- Epoch 100 error on validation set: 0.0144\n",
      "Model -- Epoch 200 error on validation set: 0.0100\n",
      "Model -- Epoch 300 error on validation set: 0.0144\n",
      "Model -- Epoch 18 has lowest error!\n",
      "(2, 1) (2, 1)\n",
      "========binary_new shape is torch.Size([1, 50])\n",
      "========cal new score\n",
      "========scores shape is 0\n",
      "========shape:  torch.Size([18, 50]) torch.Size([18, 1]) (2, 50) (2, 1)\n",
      "lr:  0.001\n",
      "Model -- Epoch 0 error on validation set: 0.2315\n",
      "Model -- Epoch 100 error on validation set: 0.0249\n",
      "Model -- Epoch 200 error on validation set: 0.0186\n",
      "Model -- Epoch 300 error on validation set: 0.0217\n",
      "Model -- Epoch 10 has lowest error!\n",
      "(2, 1) (2, 1)\n",
      "========binary_new shape is torch.Size([1, 50])\n",
      "========cal new score\n",
      "========scores shape is 0\n",
      "========shape:  torch.Size([18, 50]) torch.Size([18, 1]) (2, 50) (2, 1)\n",
      "lr:  0.001\n",
      "Model -- Epoch 0 error on validation set: 0.1671\n",
      "Model -- Epoch 100 error on validation set: 0.0062\n",
      "Model -- Epoch 200 error on validation set: 0.0069\n",
      "Model -- Epoch 300 error on validation set: 0.0067\n",
      "Model -- Epoch 36 has lowest error!\n",
      "(2, 1) (2, 1)\n",
      "========binary_new shape is torch.Size([1, 50])\n",
      "========cal new score\n",
      "========scores shape is 0\n",
      "========shape:  torch.Size([18, 50]) torch.Size([18, 1]) (2, 50) (2, 1)\n",
      "lr:  0.001\n",
      "Model -- Epoch 0 error on validation set: 0.2579\n",
      "Model -- Epoch 100 error on validation set: 0.0172\n",
      "Model -- Epoch 200 error on validation set: 0.0183\n",
      "Model -- Epoch 300 error on validation set: 0.0235\n",
      "Model -- Epoch 12 has lowest error!\n",
      "(2, 1) (2, 1)\n",
      "========binary_new shape is torch.Size([1, 50])\n",
      "========cal new score\n",
      "========scores shape is 1\n",
      "X_train shape: (19, 50), y_train shape: (19, 1)\n",
      "========shape:  torch.Size([18, 50]) torch.Size([18, 1]) (2, 50) (2, 1)\n",
      "lr:  0.001\n",
      "Model -- Epoch 0 error on validation set: 0.2366\n",
      "Model -- Epoch 100 error on validation set: 0.0135\n",
      "Model -- Epoch 200 error on validation set: 0.0184\n",
      "Model -- Epoch 300 error on validation set: 0.0236\n",
      "Model -- Epoch 34 has lowest error!\n",
      "(2, 1) (2, 1)\n",
      "========binary_new shape is torch.Size([1, 50])\n",
      "========cal new score\n",
      "========scores shape is 2\n",
      "X_train shape: (20, 50), y_train shape: (21, 1)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig/config.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     11\u001b[0m     configs \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(f)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmolecules\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 451\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(X_train, y_train, X_test, y_test, smiles, targets, model, parameters, configs, metric, seed)\u001b[0m\n\u001b[1;32m    447\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m bVAE_IM(smiles\u001b[38;5;241m=\u001b[39msmiles, targets\u001b[38;5;241m=\u001b[39mtargets, bvae_model\u001b[38;5;241m=\u001b[39mmodel, seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[1;32m    449\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 451\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning Time: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time))\n",
      "Cell \u001b[0;32mIn[2], line 137\u001b[0m, in \u001b[0;36mbVAE_IM.optimize\u001b[0;34m(self, X_train, y_train, X_test, y_test, configs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     solution, energy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solve_qubo(qubo \u001b[38;5;241m=\u001b[39m qubo,\n\u001b[1;32m    134\u001b[0m                             qubo_solver \u001b[38;5;241m=\u001b[39m solver)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# merge new data into dataset\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43msolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m                \u001b[49m\u001b[43menergy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menergy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m result_save_dir \u001b[38;5;241m=\u001b[39m configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopt\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(result_save_dir):\n",
      "Cell \u001b[0;32mIn[2], line 436\u001b[0m, in \u001b[0;36mbVAE_IM._update\u001b[0;34m(self, solution, energy)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_train shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, y_train shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# for i in range(len(b_valid_smiles)):\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m#     line = \" \".join([b_valid_smiles[i], b_full_rxn_strs[i], str(scores[i])])\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;66;03m#     writer.write(line + \"\\n\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# print(self.X_train.shape, self.y_train.shape, np.array(scores)[ :, None ].shape)\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# logging.info(\"Iteration %d: QUBO energy -- %.4f, actual energy -- %.4f, smiles -- %s\" % (self.iteration, energy[0]))\u001b[39;00m\n\u001b[0;32m--> 436\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "permutation = np.random.choice(n, n, replace = False)\n",
    "X_train = latents[ permutation, : ][ 0 : int(np.round(0.9 * n)), : ]\n",
    "X_test = latents[ permutation, : ][ int(np.round(0.9 * n)) :, : ]\n",
    "y_train = -scores[ permutation ][ 0 : int(np.round(0.9 * n)) ]\n",
    "y_test = -scores[ permutation ][ int(np.round(0.9 * n)) : ]\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "parameters =[]\n",
    "\n",
    "with open('config/config.yaml','r') as f:\n",
    "    configs = yaml.safe_load(f)\n",
    "\n",
    "main(X_train, y_train, X_test, y_test, molecules, -scores, model, parameters, configs, metric, seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
